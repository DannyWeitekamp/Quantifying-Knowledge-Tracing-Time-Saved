{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d27b3982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>p {font-size: 18px; font-family: \"Monaco\"}</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html \n",
    "<style type='text/css'>p {font-size: 18px; font-family: \"Monaco\"}</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688e79fa",
   "metadata": {},
   "source": [
    "# Estimating Time Saved by a 5% better Student Model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f756dd",
   "metadata": {},
   "source": [
    "&emsp; Knowledge tracers are typically fit to student data in the form of seqeuences of binary values indicating whether or not a student performed correctly at a particular practice opportunity. These datasets are typically fit using statistical models like Baysian Knowledge Tracing (a kind of hidden Markov Model), mixed effect logistic regression models like the Additive Factors Model (AFM), or with deep-learning (i.e. DKT). These models predict the probability that a student will get their next problem attempt correct. In this calculation we'll assume to know the ground-truth probabilities---an assumption which allows us to analytically calculate the expected Mean Square Error between a candidate model and the ground-truth one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500c2d16",
   "metadata": {},
   "source": [
    "## Calculating Expected RMSE \n",
    "\n",
    "The Mean-Square Error (MSE) for these models is typically expressed as the Brier Score:\n",
    "    \n",
    "$BS = MSE = \\frac{1}{N}\\sum_{t=1}^{N}(\\hat{p}_t-o_t)^2$\n",
    "    \n",
    "Which compares the individual continuous probability predicted by the model $\\hat{p}_t$ with the real 0 or 1 observations $o_t$.\n",
    "\n",
    "For binary random variable $X$ with ground-truth probability $P(X=1) = p$ and a model prediction $\\hat{p}$ we can express the expected MSE as:\n",
    "\n",
    "$E[MSE] = E[(\\hat{p}-X)^2]$\n",
    "\n",
    "$= E[\\hat{p}^2] - E[2\\hat{p}X] + E[(X)^2]$\n",
    "\n",
    "$= \\hat{p}^2 - 2\\hat{p}E[X] + \\frac{1}{N}\\sum[1^2p + 0^2(1-p)]$\n",
    "\n",
    "$= \\hat{p}^2 - 2\\hat{p}p + p$\n",
    "\n",
    "$E[RMSE] = \\sqrt{\\hat{p}^2 - 2\\hat{p}p + p}$\n",
    "\n",
    "(alternative derivation: https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/met.21)\n",
    "\n",
    "Let's define Python functions for the analytical MSE and RMSE, plus some helper functions to help translate between log-odds and probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6d52461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Log Likelihood\n",
    "ll = lambda p : np.log(p/(1-p))\n",
    "# Sigmoid: i.e. inverse log-likelihood\n",
    "sig = lambda l : 1/(1+np.exp(-l))\n",
    "\n",
    "# E[MSE] (i.e. Expected Brier Score) Expressed as E[(_p-X)**2] => _p**2 + p - 2*_p*p\n",
    "E_mse = lambda p,_p : _p**2 + p - 2*_p*p\n",
    "E_rmse = lambda p,_p : np.sqrt(E_mse(p,_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31a61a3",
   "metadata": {},
   "source": [
    "## The Calculating Expect Time Saved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca07776",
   "metadata": {},
   "source": [
    "### Find the old 5% worse threshold intercept "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a79a753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A mastery threshold intercept of 95%\n",
    "thresh = .95\n",
    "old_p_at_thresh = .8785 #Here we're assuming an underestimate of mastery\n",
    "\n",
    "# Ensure this choice the old threshold intercept has 5% worse RMSE than the new generating model.\n",
    "assert np.abs(E_rmse(thresh, old_p_at_thresh)*.95-E_rmse(thresh,thresh))  < 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c0337e",
   "metadata": {},
   "source": [
    "### Set Ground-Truth Parameters\n",
    "\n",
    "Below we're assuming that the model is a logistic regression model like AFM. We'll set a ground-truth intercept `new_intr` and slope `new_slope`. Since we're doing a simple back-of-the-napkin sort of calculation these two parameters are considerably simplified compared to the typical parameterization where there is a per-student intercept paramater, and a per KC intercept and slope. With just two paramers we're only modeling an average set of parameters consistent with the properties of a typical dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c8f213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground-truth parameters (1) stu intercept .35 (2) avg 12 problems to mastery\n",
    "intr_p = .35\n",
    "avg_probs = 12\n",
    "\n",
    "#Model ground-truth model parameters  \n",
    "new_intr = ll(intr_p)\n",
    "new_slope = (ll(thresh)-new_intr)/avg_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060d387a",
   "metadata": {},
   "source": [
    "### Calculate the parameters of the old model running through this intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72d4215d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept old/new: -0.785  -0.619\n",
      "slope old/new: \t    0.230   0.297\n"
     ]
    }
   ],
   "source": [
    "def gen_old_model_params(old_p_at_thresh):\n",
    "    '''Generates the intercept and slope for the 'old' model assuming a particular\n",
    "       predicted probability of the old model after 'avg_probs' '''\n",
    "    # The probability difference at the threshold intercept\n",
    "    p_diff = thresh-old_p_at_thresh\n",
    "\n",
    "    # Half the improvement from old to new was due to intercept change\n",
    "    old_intr = .5 * ll(intr_p-p_diff) + \\\n",
    "               .5 * new_intr\n",
    "    # Adjust the slope to account for the rest of the change\n",
    "    old_slope = (ll(old_p_at_thresh)-old_intr)/avg_probs \n",
    "    \n",
    "    # Ensure that these parameters reproduce 'old_p_at_thresh'\n",
    "    assert np.isclose(sig(old_intr+avg_probs*old_slope), old_p_at_thresh)\n",
    "    return old_intr, old_slope\n",
    "\n",
    "old_intr, old_slope = gen_old_model_params(old_p_at_thresh)\n",
    "print(f\"intercept old/new: {old_intr:.3f}  {new_intr:.3f}\")\n",
    "print(f\"slope old/new: \\t    {old_slope:.3f}   {new_slope:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cd6d09",
   "metadata": {},
   "source": [
    "### Then Interpolate the time saved by the new model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ef64e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_extra_probs_old_model: 4.19\n",
      "Total Time Saved: 524.36 minutes or 8.74 hours. (34.96)% of 25.0 total hours\n"
     ]
    }
   ],
   "source": [
    "avg_probs_old = (ll(thresh)-old_intr)/old_slope\n",
    "print(f\"avg_extra_probs_old_model: {avg_probs_old-avg_probs:.2f}\")\n",
    "\n",
    "attempt_length = 15 #seconds\n",
    "n_kcs = 500\n",
    "total_time_saved = (avg_probs_old-avg_probs)*n_kcs*attempt_length\n",
    "total_time = n_kcs*attempt_length*avg_probs\n",
    "print(f\"Total Time Saved: {total_time_saved/60:.2f} minutes or {total_time_saved/3600:.2f} hours. ({100*total_time_saved/total_time:.2f})% of {total_time/3600} total hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4de7674",
   "metadata": {},
   "source": [
    "&emsp; In total a the 5% better model gives us a rather large expected time saving of 35%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d55bfa3",
   "metadata": {},
   "source": [
    "## What about whole model RMSE?\n",
    "\n",
    "&emsp; We can do a similar calculation for the RMSE of the whole model (not just at the threshold point). However, the RMSE outside of the neighborhood of the mastery threshold has no bearing on the problem selection behavior of a knowledge-tracer. Nonetheless in this case if we tweak the parameters so that the RMSE difference of the whole model is 5% then, we get a similar RMSE difference in the neighborhood of the mastery threshold. It's hard to say if this is true in general or  a quirk of comparing two versions of the same model: one with ground-truth parameters and one with perturbed parameters. The more typical comparison would be to compare two models with different choices of independant variables, like Performance Factors Analysis (PFA) versus Additive Factors Model (AFM), or a different model variety entirely like Baysian Knowledge Tracing (BKT) or Deep Knowledge Tracing (DKT).  The reality is that we really ought to be in the habit of reporting model performance statistics around reasonable threshold choices (i.e. 85%, 90%, 95%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a260969e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual RMSE difference at mastery threshold: 4.83%\n",
      "old/new intr.: \t-0.782  -0.619\n",
      "old/new slope: \t0.231  0.297\n",
      "avg_extra_probs_old_model: 4.13\n",
      "Total Time Saved: 516.68 minutes or 8.61 hours. (34.45)% of 25.0 total hours\n"
     ]
    }
   ],
   "source": [
    "def E_model_rmse(intr, slope, opp_cutoff=15):\n",
    "    ''' Given intr and slope generate the expected RMSE of the model compared to ground-truth'''\n",
    "    p_ground_truth = lambda i : sig(new_intr+i*new_slope)\n",
    "    p_model = lambda i : sig(intr+i*slope)\n",
    "    mse_s = []\n",
    "    for i in range(opp_cutoff+1):\n",
    "        mse_s.append(np.mean([E_mse(p_ground_truth(i), p_model(i)) ]))\n",
    "    return np.mean(mse_s)\n",
    "\n",
    "old_p_at_thresh = .8797\n",
    "old_intr, old_slope = gen_old_model_params(old_p_at_thresh)\n",
    "\n",
    "assert np.abs(E_model_rmse(old_intr, old_slope)*.95-E_model_rmse(new_intr,new_slope)) < 1e-4\n",
    "\n",
    "print(f\"Actual RMSE difference at mastery threshold: {100*(1-E_rmse(thresh, thresh)/E_rmse(thresh, old_p_at_thresh)):.2f}%\" )\n",
    "\n",
    "print(f\"old/new intr.: \\t{old_intr:.3f}  {new_intr:.3f}\")\n",
    "print(f\"old/new slope: \\t{old_slope:.3f}  {new_slope:.3f}\")\n",
    "\n",
    "avg_probs_old = (ll(thresh)-old_intr)/old_slope\n",
    "print(f\"avg_extra_probs_old_model: {avg_probs_old-avg_probs:.2f}\")\n",
    "\n",
    "attempt_length = 15 #seconds\n",
    "n_kcs = 500\n",
    "total_time_saved = (avg_probs_old-avg_probs)*n_kcs*attempt_length\n",
    "print(f\"Total Time Saved: {total_time_saved/60:.2f} minutes or {total_time_saved/3600:.2f} hours. ({100*total_time_saved/total_time:.2f})% of {total_time/3600} total hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8b76dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22924\n",
      "0.22937\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import random as rand\n",
    "\n",
    "# Sanity Check : Does analytic expected RMSE match an empirically generate one\n",
    "N = 10000000\n",
    "print(f'{np.sqrt(np.mean( np.square( (rand(N)<=.95).astype(np.float64) - .8785 ))):.5f}')\n",
    "print(f'{E_rmse(thresh, .8785):.5f}')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
